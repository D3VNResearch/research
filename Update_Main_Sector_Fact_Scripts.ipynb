{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12012f35",
   "metadata": {},
   "source": [
    "### IMPORT LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378258c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from office365.runtime.auth.authentication_context import AuthenticationContext\n",
    "from office365.sharepoint.client_context import ClientContext\n",
    "from office365.sharepoint.files.file import File\n",
    "from office365.runtime.auth.client_credential import ClientCredential\n",
    "from office365.runtime.client_request_exception import ClientRequestException\n",
    "import datetime\n",
    "import pytz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import glob\n",
    "import pyodbc\n",
    "import os \n",
    "import json\n",
    "from io import BytesIO\n",
    "import io\n",
    "import platform\n",
    "from function.PyToSp import *\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import quote_plus\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, event\n",
    "import pyodbc\n",
    "import requests\n",
    "import inspect\n",
    "from validate_update import *\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "from urllib.parse import quote_plus\n",
    "import msal\n",
    "from itertools import chain\n",
    "from termcolor import colored\n",
    "from send_email import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bee114",
   "metadata": {},
   "source": [
    "### CONNECT TO AZURE SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39960d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open ('database_information.json', \"r\")\n",
    "qq = json.loads(f.read())\n",
    "f.close()\n",
    "ini_cnt_str ='Driver={driver_str};Server=tcp:hkazdevsqld3vnreserch.database.windows.net,1433;database={database};Uid={username};Pwd={password};Encrypt=yes;Authentication=ActiveDirectoryPassword;Connection Timeout=30;'.format(**qq)\n",
    "quoted = quote_plus(ini_cnt_str)\n",
    "cnt_str = 'mssql+pyodbc:///?odbc_connect={}'.format(quoted)\n",
    "engine = create_engine(cnt_str)\n",
    "\n",
    "#Test ConnectionD\n",
    "try:\n",
    "    result = engine.execute(\"SELECT 1\")\n",
    "    print(\"CONNECTION SUCESSFUL!\")\n",
    "except Exception as e:\n",
    "    print(\"CONNECTION FAILED:\",str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81420039",
   "metadata": {},
   "source": [
    "### CONNECT TO SHAREPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0587ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_BIHub = 'share_point_BIHub'\n",
    "config_BIHub = read_config_json(config_path, header_BIHub)\n",
    "BIHub = SharePoint(config_BIHub)\n",
    "BIHub.check_connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e642c12",
   "metadata": {},
   "source": [
    "### READ AND LIST FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d951e70e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Tất cả các tỉnh\n",
    "relative_url = \"/sites/BIHub/Shared Documents/Advisory Data/Main sector/Update/Fact\"\n",
    "sp_object = relative_url.split('/')[2].replace('-','')\n",
    "list_folder = eval(sp_object).get_content_url(relative_url,return_list_folder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162f57a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lấy năm\n",
    "list_folder_sub1=[]\n",
    "for i in list_folder:\n",
    "    ls=eval(sp_object).get_content_url(i,return_list_folder=True)\n",
    "    list_folder_sub1=list_folder_sub1+ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338da9f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lấy quý\n",
    "list_folder_sub2=[]\n",
    "for i in list_folder_sub1:\n",
    "    ls=eval(sp_object).get_content_url(i,return_list_folder=True)\n",
    "    list_folder_sub2=list_folder_sub2+ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa46fe3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Provide information\n",
    "Province_name=['Binh Duong Province']#Chọn tỉnh cần import-- là tên folder \n",
    "Year=['2023']#Chọn năm cần import\n",
    "Quarter=['Q1']#Chọn quý cần import\n",
    "#Có thể chọn nhiều tỉnh, tên tỉnh để trong dấu nháy đơn và cách nhau bởi dấy phẩy. Áp dụng tương tự cho năm, quý\n",
    "#-------------------------------------------------------\n",
    "df_summ_file = pd.DataFrame({'Name':[],'ServerRelativeUrl':[], 'TimeLastModified':[], 'ModTime':[], 'Modified_by_ID':[]})\n",
    "for i in list_folder_sub2:\n",
    "    if i.split('/')[-3] in Province_name and i.split('/')[-2] in Year and i.split('/')[-1] in Quarter:\n",
    "        df_summ_file = pd.concat([df_summ_file, eval(sp_object).get_content_url(i)])\n",
    "list_url = df_summ_file['ServerRelativeUrl'].to_list()\n",
    "#History file\n",
    "df_query=pd.DataFrame(df_summ_file).reset_index(drop=True)\n",
    "df_summ_file = df_summ_file.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbe169c",
   "metadata": {},
   "source": [
    "### GET IMPORT FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e0061a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = []\n",
    "for i in list_url:\n",
    "    sector = i.split('/')[-1].split('_')[1].upper()\n",
    "    if sector in ('VLTH'): #Chọn sector cần import: RETAIL, OFFICE, SA, HOTEL, APT, VLTH\n",
    "        url.append(i)\n",
    "    else:\n",
    "        pass\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df571cf7",
   "metadata": {},
   "source": [
    "### IMPORT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa7bc94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Prepare ingredients''' \n",
    "columns_that_need_unidecode = ['Sub_Project_Name', 'Project_City_Name', 'Project_District_Name']\n",
    "#Create empty df for checking dictionary\n",
    "df_dict = pd.DataFrame(columns=['File_Name', 'Missing_Values', 'Flag'])\n",
    "name_sector = ['RETAIL', 'SA', 'APT', 'VLTH', 'OFFICE', 'HOTEL']\n",
    "for i in name_sector:\n",
    "    globals()[f'df_update_{i.lower()}'] = pd.DataFrame([])\n",
    "    globals()[f'df_flat__{i.lower()}'] = pd.DataFrame([])\n",
    "#-------------------------------------------------------\n",
    "'''Get data''' \n",
    "for file_url in tqdm(url):\n",
    "    data, file_name, sector = get_data(relative_url, file_url)\n",
    "    # #-------------------------------------------------------\n",
    "\n",
    "    '''Validation step'''\n",
    "    #Remove unfortmated values\n",
    "    data = remove_unformated_character(data)\n",
    "\n",
    "    #Remove unicode characters\n",
    "    for i in columns_that_need_unidecode:\n",
    "        data[i] = remove_unicode(data[i])\n",
    "    #Check dictionary\n",
    "    lst_dict = ['City', 'District', 'Type', 'Grade']\n",
    "    lst_cls = ['Project_City_Name', 'Project_District_Name', 'Sub_Project_Type', 'Grade']\n",
    "    for i, j in zip(lst_cls, lst_dict):\n",
    "        data, df_dict = check_dictionary(df_dict, file_name, data, i, j, sector, engine, sp_object)\n",
    "    if len(df_dict) == 0:\n",
    "        print(colored('Validate succesfully','green'))\n",
    "        #-------------------------------------------------------\n",
    "        '''Import data process'''\n",
    "        #Check project key\n",
    "        processed_data, flag_key = check_project_key(file_url, data, sector, engine)\n",
    "        if flag_key == 0:\n",
    "            data = get_project_key(flag_key, processed_data, data, sector, engine)\n",
    "            if sector == 'RETAIL':\n",
    "                df_update_retail = pd.concat([df_update_retail, data], axis=0)\n",
    "                df_flat_retail = get_flat_file(df_update_retail, sector, engine)\n",
    "                flag_update, data = update_flat_file(sector, df_flat_retail, df_update_retail)\n",
    "            elif sector == 'OFFICE':\n",
    "                df_update_office = pd.concat([df_update_office, data], axis=0)\n",
    "                df_flat_office = get_flat_file(df_update_office, sector, engine)\n",
    "                flag_update, data = update_flat_file(sector, df_flat_office, df_update_office)\n",
    "            elif sector == 'HOTEL':\n",
    "                df_update_hotel = pd.concat([df_update_hotel, data], axis=0)\n",
    "                df_flat_hotel = get_flat_file(df_update_hotel, sector, engine)\n",
    "                flag_update, data = update_flat_file(sector, df_flat_hotel, df_update_hotel)\n",
    "            elif sector == 'SA' or sector=='SERVICED_APARTMENT':\n",
    "                df_update_sa = pd.concat([df_update_sa, data], axis=0)\n",
    "                df_flat_sa = get_flat_file(df_update_sa, sector, engine)\n",
    "                flag_update, data = update_flat_file(sector, df_flat_sa, df_update_sa)\n",
    "            elif sector == 'APT' or sector=='APARTMENT':\n",
    "                df_update_apt = pd.concat([df_update_apt, data], axis=0)\n",
    "                df_flat_apt = get_flat_file(df_update_apt, sector, engine)\n",
    "                flag_update, data = update_flat_file(sector, df_flat_apt, df_update_apt)\n",
    "            elif sector == 'VLTH':\n",
    "                df_update_vlth = pd.concat([df_update_vlth, data], axis=0)\n",
    "                df_flat_vlth = get_flat_file(df_update_vlth, sector, engine)\n",
    "                flag_update, data = update_flat_file(sector, df_flat_vlth, df_update_vlth)\n",
    "            #-------------------------------------------------------    \n",
    "            if flag_update == 0:\n",
    "#                 insert_to_fresh(file_url, data, cnt_str)\n",
    "                list_df_update = [df_update_retail, df_update_office, df_update_hotel, df_update_sa, df_update_apt, df_update_vlth]\n",
    "                for df_update in list_df_update:\n",
    "                    if len(df_update) != 0:\n",
    "                        df_update = Generate_Additional_Columns(df_update,url,df_summ_file,BIHub,engine,file_url)\n",
    "                        insert_to_tracking(df_update, sector, engine)\n",
    "            else:\n",
    "                print(colored(f'Some keys wrong in {file_name}', 'yellow'))\n",
    "        else:\n",
    "            print(colored(f'Cant get key in {file_name}', 'yellow'))\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f32c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test check_project_key\n",
    "'''Prepare ingredients''' \n",
    "columns_that_need_unidecode = ['Sub_Project_Name', 'Project_City_Name', 'Project_District_Name']\n",
    "#Create empty df for checking dictionary\n",
    "df_dict = pd.DataFrame(columns=['File_Name', 'Missing_Values', 'Flag'])\n",
    "name_sector = ['RETAIL', 'SA', 'APT', 'VLTH', 'OFFICE', 'HOTEL']\n",
    "for i in name_sector:\n",
    "    globals()[f'df_update_{i.lower()}'] = pd.DataFrame([])\n",
    "    globals()[f'df_flat__{i.lower()}'] = pd.DataFrame([])\n",
    "#-------------------------------------------------------\n",
    "'''Get data''' \n",
    "for file_url in tqdm(url):\n",
    "    data, file_name, sector = get_data(relative_url, file_url)\n",
    "    # #-------------------------------------------------------\n",
    "\n",
    "    '''Validation step'''\n",
    "    #Remove unfortmated values\n",
    "    data = remove_unformated_character(data)\n",
    "\n",
    "    #Remove unicode characters\n",
    "    for i in columns_that_need_unidecode:\n",
    "        data[i] = remove_unicode(data[i])\n",
    "    #Check dictionary\n",
    "    lst_dict = ['City', 'District', 'Type', 'Grade']\n",
    "    lst_cls = ['Project_City_Name', 'Project_District_Name', 'Sub_Project_Type', 'Grade']\n",
    "    for i, j in zip(lst_cls, lst_dict):\n",
    "        data, df_dict = check_dictionary(df_dict, file_name, data, i, j, sector, engine, sp_object)\n",
    "    if len(df_dict) == 0:\n",
    "        print(colored('Validate succesfully','green'))\n",
    "        #-------------------------------------------------------\n",
    "        '''Import data process'''\n",
    "\n",
    "list_keys = ['Sector', 'Sub_Project_Name', 'Project_District_Name','Project_City_Name','Project_Type','Project_Key']\n",
    "list_keys_on =['Sector', 'Sub_Project_Name','Project_District_Name','Project_City_Name','Project_Type']\n",
    "table_name = 'Main_Sector_Project'\n",
    "lower_function = lambda x: x.strip().lower() if isinstance(x, str) else x\n",
    "if sector in ['VLTH', 'RETAIL']:\n",
    "    data = data.rename(columns = {'Sub_Project_Type':'Project_Type'})\n",
    "\n",
    "if sector == 'VLTH':\n",
    "    if 'Branded_Flag' in data.columns:\n",
    "        df_query = pd.read_sql(f\"select * from FRESH.{table_name} WHERE Branded_Flag = 1\",engine)\n",
    "    else:\n",
    "        df_query = pd.read_sql(f\"select * from FRESH.{table_name}\",engine)\n",
    "data[list_keys_on] = data[list_keys_on].applymap(lower_function)\n",
    "df_query[list_keys] = df_query[list_keys].applymap(lower_function)\n",
    "merged_data = pd.merge(data, df_query[list_keys],\n",
    "                        left_on = list_keys_on,\n",
    "                        right_on = list_keys_on,\n",
    "                        how = 'left'\n",
    "                    )\n",
    "# merged_data                    \n",
    "processed_data = merged_data[merged_data['Project_Key'].isnull()]\n",
    "processed_data.count().sum()\n",
    "\n",
    "### End check_project_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c988e0",
   "metadata": {},
   "source": [
    "# Test update_flat_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdf0264",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat = df_flat_vlth\n",
    "df_flat.head()\n",
    "# df_update = df_update_vlth\n",
    "# flag_update = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eed68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_update = df_update_vlth\n",
    "df_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387c82e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_update\n",
    "flag_update = 0\n",
    "list_keys_on = ['Project_Key', 'Date_Key']\n",
    "if sector == 'RETAIL':\n",
    "    list_update = ['Project_Key', 'Date_Key', 'Form', 'Project_Status'\n",
    "                    , 'GFA', 'NLA', 'Vacant_Area', 'Leased_Area'\n",
    "                    , 'Avg_Gross_Rent', 'New_Supply'\n",
    "                    ]\n",
    "elif sector == 'OFFICE':\n",
    "    list_update = ['Project_Key', 'Date_Key', 'Form', 'Project_Status'\n",
    "                    , 'NLA', 'Leased_Area_End_Of_Q', 'Vacant_Area'\n",
    "                    , 'Avg_Net_Rent', 'Avg_Gross_Rent', 'New_Supply'\n",
    "                    ]\n",
    "elif sector == 'HOTEL':\n",
    "    list_update = ['Project_Key', 'Date_Key', 'Form', 'Project_Status'\n",
    "                    , 'Total_Supply', 'Leased_Rooms', 'Avg_Room_Price'\n",
    "                    , 'Quoted_Room_Rate', 'Occupancy', 'New_Supply'\n",
    "                    ]\n",
    "elif sector == 'SERVICED_APARTMENT' or sector == 'SA':\n",
    "    list_update = ['Project_Key', 'Date_Key', 'Form', 'Project_Status'\n",
    "                    , 'Total_Supply', 'Launch_Units', 'Leased_Units', 'Vacant_Units'\n",
    "                    , 'Avg_Rent', 'Net_Lettable', 'Rev_PAU', 'Rev_PAU_m2', 'Rent_Per_Unit', 'New_Supply'\n",
    "                    ]\n",
    "elif sector == 'APT' or sector=='APARTMENT':\n",
    "    list_update = ['Project_Key', 'Date_Key', 'Form', 'Project_Status'\n",
    "                    , 'Total_Supply', 'Secondary_Supply', 'Newly_Launch', 'Total_Launched_Units'\n",
    "                    , 'Available_Units', 'Total_Sold_Units','Sold_Units_End_Of_Q', 'Quarterly_Sales'\n",
    "                    , 'Remaining_Supply', 'NFA_Primary_Price', 'NSA_Primary_Price', 'UNIT_Primary_Price'\n",
    "                    , 'Avg_Primary_Price', 'NFA_Secondary_Price', 'NSA_Secondary_Price', 'UNIT_Secondary_Price'\n",
    "                    , 'Avg_Secondary_Price', 'Quarterly_Sales_New_Supply'\n",
    "                    ]\n",
    "elif sector == 'VLTH':\n",
    "    list_update = ['Project_Key', 'Date_Key', 'Form', 'Project_Status'\n",
    "                    , 'Total_Supply', 'Newly_Launch', 'Total_Launched_Units', 'Available_Units'\n",
    "                    , 'Total_Sold_Units', 'Sold_Units_End_Of_Q', 'Quarterly_Sales', 'Remaining_Supply'\n",
    "                    , 'LA_Primary_Price', 'GFA_Primary_Price', 'UNIT_Primary_Price', 'LA_Secondary_Price'\n",
    "                    , 'GFA_Secondary_Price', 'UNIT_Secondary_Price', 'Quarterly_Sales_New_Supply'\n",
    "                    ]\n",
    "elif sector == 'IP':\n",
    "    list_update = ['Project_Key', 'Date_Key', 'Form', 'Status'\n",
    "                    , 'PROJECT_Land_Area', 'LAND_Leaseable_Land_Area', 'LAND_Available_Land_Area', 'LAND_Leased_Land_Area'\n",
    "                    , 'RB_Land_Area', 'RB_Total_GFA', 'RB_NLA', 'RB_Leased_Area', 'Occupancy'\n",
    "                    , 'LAND_AVG_Rent_Primary_Price', 'LAND_AVG_Rent_Secondary_Price', 'LAND_AVG_Rent_Actual'\n",
    "                    , 'LAND_AVG_Rent_Last_Transaction', 'RB_AVG_Rent_Primary_Price', 'RB_AVG_Rent_Secondary_Price'\n",
    "                    , 'RB_AVG_Rent_Actual', 'RB_AVG_Rent_Last_Transaction'\n",
    "                    ]\n",
    "list_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a53cd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_function = lambda x: int(x) if isinstance(x, str) else x\n",
    "df_update[list_keys_on] = df_update[list_keys_on].applymap(int_function)\n",
    "df_flat[list_keys_on] = df_flat[list_keys_on].applymap(int_function)\n",
    "merge_how = ['left', 'inner']    \n",
    "for i in merge_how:\n",
    "    globals()[f'df_merge_{i.lower()}'] = pd.merge(df_update, df_flat[list_update]\n",
    "                            , left_on = list_keys_on\n",
    "                            , right_on = list_keys_on\n",
    "                            , how = i\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c043f2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_left[list_keys_on]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456d9a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_inner[list_keys_on]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4dca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_gap = df_merge_left[~df_merge_left['Project_Key'].isin(df_merge_inner['Project_Key'])]\n",
    "column_gap = ['Project_Key','Date_Key','Sub_Project_Name']\n",
    "df_merge_gap[column_gap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d363d802",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
