{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f9c70bc",
   "metadata": {},
   "source": [
    "### IMPORT LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccfa4d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from office365.runtime.auth.authentication_context import AuthenticationContext\n",
    "from office365.sharepoint.client_context import ClientContext\n",
    "from office365.sharepoint.files.file import File\n",
    "from office365.runtime.auth.client_credential import ClientCredential\n",
    "from office365.runtime.client_request_exception import ClientRequestException\n",
    "import datetime\n",
    "import pytz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import glob\n",
    "import pyodbc\n",
    "import os \n",
    "import json\n",
    "from io import BytesIO\n",
    "import io\n",
    "import platform\n",
    "from function.PyToSp import *\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import quote_plus\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, event\n",
    "import pyodbc\n",
    "import requests\n",
    "import inspect\n",
    "from validate import *\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "from urllib.parse import quote_plus\n",
    "import msal\n",
    "from itertools import chain\n",
    "from send_email import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8bee114",
   "metadata": {},
   "source": [
    "### CONNECT TO AZURE SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a39960d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open ('database_information.json', \"r\")\n",
    "qq = json.loads(f.read())\n",
    "f.close()\n",
    "ini_cnt_str ='Driver={driver_str};Server=tcp:hkazdevsqld3vnreserch.database.windows.net,1433;database={database};Uid={username};Pwd={password};Encrypt=yes;Authentication=ActiveDirectoryPassword;Connection Timeout=30;'.format(**qq)\n",
    "quoted = quote_plus(ini_cnt_str)\n",
    "cnt_str = 'mssql+pyodbc:///?odbc_connect={}'.format(quoted)\n",
    "engine = create_engine(cnt_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81420039",
   "metadata": {},
   "source": [
    "### CONNECT TO SHAREPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b0587ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web site title: S22M Research & BI Hub _ Successful Connection!\n"
     ]
    }
   ],
   "source": [
    "header_BIHub = 'share_point_BIHub'\n",
    "config_BIHub = read_config_json(config_path, header_BIHub)\n",
    "BIHub = SharePoint(config_BIHub)\n",
    "BIHub.check_connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d951e70e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder name: /sites/BIHub/Shared Documents/Advisory Data/Macro commentary/Flat file/2022\n"
     ]
    }
   ],
   "source": [
    "#Tất cả các tỉnh\n",
    "relative_url = \"/sites/BIHub/Shared Documents/Advisory Data/Macro commentary/Flat file\"\n",
    "sp_object = relative_url.split('/')[2].replace('-','')\n",
    "list_folder = eval(sp_object).get_content_url(relative_url,return_list_folder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "955b3c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder name: /sites/BIHub/Shared Documents/Advisory Data/Macro commentary/Flat file/2022/Q1\n",
      "/sites/BIHub/Shared Documents/Advisory Data/Macro commentary/Flat file/2022/Q2\n"
     ]
    }
   ],
   "source": [
    "#lấy năm\n",
    "list_folder_sub1=[]\n",
    "for i in list_folder:\n",
    "    ls=eval(sp_object).get_content_url(i,return_list_folder=True)\n",
    "    list_folder_sub1=list_folder_sub1+ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa46fe3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder name: \n",
      "Files name: /sites/BIHub/Shared Documents/Advisory Data/Macro commentary/Flat file/2022/Q1/Macro_20220331.xlsx\n"
     ]
    }
   ],
   "source": [
    "quarter = ['Q1']#Chọn năm cần import\n",
    "year = ['2022']#Chọn quý cần import\n",
    "#-------------------------------------------------------\n",
    "df_summ_file = pd.DataFrame({'Name':[],'ServerRelativeUrl':[], 'TimeLastModified':[], 'ModTime':[], 'Modified_by_ID':[]})\n",
    "for i in list_folder_sub1:\n",
    "    if i.split('/')[-2] in year and i.split('/')[-1] in quarter:\n",
    "        df_summ_file = pd.concat([df_summ_file, eval(sp_object).get_content_url(i)])\n",
    "list_file = df_summ_file['ServerRelativeUrl'].to_list()\n",
    "#History file\n",
    "df_query=pd.DataFrame(df_summ_file).reset_index(drop=True)\n",
    "df_summ_file = df_summ_file.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5bc3144",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/sites/BIHub/Shared Documents/Advisory Data/Macro commentary/Flat file/2022/Q1/Macro_20220331.xlsx']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = []\n",
    "for i in list_file:\n",
    "    data_key = i.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "    if str(data_key) not in ('x'):\n",
    "        url.append(i)\n",
    "url        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fc83b84",
   "metadata": {},
   "source": [
    "### IMPORT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51d50056",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can not read Macro_20220331.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'dropna'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39m'''Get data'''\u001b[39;00m \n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m file_url \u001b[39min\u001b[39;00m tqdm(url):\n\u001b[1;32m----> 9\u001b[0m     data, file_name, sector \u001b[39m=\u001b[39m get_data(relative_url, file_url)\n\u001b[0;32m     10\u001b[0m     \u001b[39m#-------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     data \u001b[39m=\u001b[39m check_date_key(file_url, data)\u001b[39m#Check format date_key in flat file\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Project_Savills\\research\\validate.py:45\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(relative_url, file_url)\u001b[0m\n\u001b[0;32m     42\u001b[0m data \u001b[39m=\u001b[39m \u001b[39meval\u001b[39m(sp_object)\u001b[39m.\u001b[39mget_file(file_url)   \n\u001b[0;32m     43\u001b[0m \u001b[39m#Remove irrelevant data\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39m#data.dropna(axis = 0, how = 'all', thresh = None, subset = None, inplace = True)#remove blank rows\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m data\u001b[39m.\u001b[39;49mdropna(axis \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, how \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mall\u001b[39m\u001b[39m'\u001b[39m, inplace \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\u001b[39m#remove blank rows\u001b[39;00m\n\u001b[0;32m     46\u001b[0m data\u001b[39m.\u001b[39mdrop(data\u001b[39m.\u001b[39mcolumns[data\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mcontains(\u001b[39m'\u001b[39m\u001b[39mUnnamed\u001b[39m\u001b[39m'\u001b[39m)], axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, inplace \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\u001b[39m#remove blank columns\u001b[39;00m\n\u001b[0;32m     47\u001b[0m data\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mreplace(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms+\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, regex \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\u001b[39m#strip column\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'bool' object has no attribute 'dropna'"
     ]
    }
   ],
   "source": [
    "'''Prepare ingredients'''\n",
    "columns_that_need_unidecode=['City', 'Commentary_Image'\n",
    "                            ]\n",
    "#Create empty df for checking dictionary\n",
    "df_dict = pd.DataFrame(columns=['File_Name', 'Missing_Values', 'Flag'])\n",
    "#-------------------------------------------------------\n",
    "'''Get data''' \n",
    "for file_url in tqdm(url):\n",
    "    data, file_name, sector = get_data(relative_url, file_url)\n",
    "    #-------------------------------------------------------\n",
    "    data = check_date_key(file_url, data)#Check format date_key in flat file\n",
    "    #-------------------------------------------------------\n",
    "    '''Validation step'''\n",
    "    #Remove unfortmated values\n",
    "    data = remove_unformated_character(data)\n",
    "    #Remove unicode characters\n",
    "    for i in columns_that_need_unidecode:\n",
    "        data[i] = remove_unicode(data[i])\n",
    "    #Check dictionary\n",
    "    lst_dict = ['City']\n",
    "    lst_cls = ['City']\n",
    "    for i, j in zip(lst_cls, lst_dict):\n",
    "        data, df_dict = check_dictionary(df_dict, file_name, data, i, j, sector, engine, sp_object)\n",
    "    if len(df_dict) == 0:\n",
    "        print(colored('Validate succesfully','green'))\n",
    "        data = Generate_Additional_Columns(data,url,df_summ_file,BIHub,engine,file_url)\n",
    "        data.to_sql('Macro_Commentary', engine, index=False, if_exists='append', schema='Fresh')\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a4adc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
