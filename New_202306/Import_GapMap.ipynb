{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from office365.runtime.auth.authentication_context import AuthenticationContext\n",
    "from office365.sharepoint.client_context import ClientContext\n",
    "from office365.sharepoint.files.file import File\n",
    "from office365.runtime.auth.client_credential import ClientCredential\n",
    "from office365.runtime.client_request_exception import ClientRequestException\n",
    "import datetime\n",
    "import pytz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import glob\n",
    "import pyodbc\n",
    "import os \n",
    "import json\n",
    "from io import BytesIO\n",
    "import io\n",
    "import platform\n",
    "import openpyxl\n",
    "from function.PyToSp import *\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import quote_plus\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, event\n",
    "import pyodbc\n",
    "import requests\n",
    "import inspect\n",
    "from validateSamSung import *\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "from urllib.parse import quote_plus\n",
    "import msal\n",
    "from itertools import chain\n",
    "from send_email import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Connect to Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open ('database_information.json', \"r\")\n",
    "qq = json.loads(f.read())\n",
    "f.close()\n",
    "ini_cnt_str ='Driver={driver_str};Server=tcp:hkazdevsqld3vnreserch.database.windows.net,1433;database={database};Uid={username};Pwd={password};Encrypt=yes;Authentication=ActiveDirectoryPassword;Connection Timeout=30;'.format(**qq)\n",
    "quoted = quote_plus(ini_cnt_str)\n",
    "cnt_str = 'mssql+pyodbc:///?odbc_connect={}'.format(quoted)\n",
    "engine = create_engine(cnt_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Connect To SharePoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web site title: S22M Research & BI Hub _ Successful Connection!\n"
     ]
    }
   ],
   "source": [
    "header_BIHub = 'share_point_BIHub'\n",
    "config_BIHub = read_config_json(config_path, header_BIHub)\n",
    "BIHub = SharePoint(config_BIHub)\n",
    "BIHub.check_connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Lấy đường dẫn chứa File Gap Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder name: /sites/BIHub/Shared Documents/Advisory Data/Project/Crawling Data\n",
      "/sites/BIHub/Shared Documents/Advisory Data/Project/SAMSUNG\n",
      "/sites/BIHub/Shared Documents/Advisory Data/Project/GapMap Data\n"
     ]
    }
   ],
   "source": [
    "#Tất cả các tỉnh\n",
    "relative_url = \"/sites/BIHub/Shared Documents/Advisory Data/Project\"\n",
    "sp_object = relative_url.split('/')[2].replace('-','')\n",
    "list_folder = eval(sp_object).get_content_url(relative_url,return_list_folder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder name: /sites/BIHub/Shared Documents/Advisory Data/Project/Crawling Data/News\n",
      "/sites/BIHub/Shared Documents/Advisory Data/Project/Crawling Data/MST\n",
      "Folder name: /sites/BIHub/Shared Documents/Advisory Data/Project/SAMSUNG/Flat file\n",
      "/sites/BIHub/Shared Documents/Advisory Data/Project/SAMSUNG/Raw file\n",
      "Folder name: \n",
      "Files name: /sites/BIHub/Shared Documents/Advisory Data/Project/GapMap Data/Commune.xlsx\n",
      "Files name: /sites/BIHub/Shared Documents/Advisory Data/Project/GapMap Data/District.xlsx\n"
     ]
    }
   ],
   "source": [
    "df_summ_file = pd.DataFrame({'Name':[],'ServerRelativeUrl':[], 'TimeLastModified':[], 'ModTime':[], 'Modified_by_ID':[]})\n",
    "for i in list_folder:\n",
    "    df_summ_file = pd.concat([df_summ_file, eval(sp_object).get_content_url(i)])\n",
    "list_file = df_summ_file['ServerRelativeUrl'].to_list()\n",
    "#History file\n",
    "df_query=pd.DataFrame(df_summ_file).reset_index(drop=True)\n",
    "df_summ_file = df_summ_file.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/sites/BIHub/Shared Documents/Advisory Data/Project/GapMap Data/Commune.xlsx',\n",
       " '/sites/BIHub/Shared Documents/Advisory Data/Project/GapMap Data/District.xlsx']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = []\n",
    "for i in list_file:\n",
    "    data_key = i.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "    if str(data_key) not in ('x'):\n",
    "        url.append(i)\n",
    "url   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unformated_character_GapMap(data):\n",
    "    #re.sub(pattern, repl, string, count=0, flags=0)\n",
    "    list_unformated_character = ['#N/a', 'N/A', 'N/a', 'n/a', 'NA', '-'\n",
    "                                 , '$-', '#DIV/0!', '0', 0, 'FALSE' \n",
    "                                 , 'Null'\n",
    "                                ]\n",
    "    for columns in data.columns:\n",
    "        data[columns] = data[columns].replace(r'^\\s*$', np.nan, regex=True)#Replacing blank values (has white space)\n",
    "        data[columns] = data.loc[:,columns].fillna(value='NULL')\n",
    "        data[columns] = data[columns].replace('(^\\s+|\\s+$)', '', regex=True)\n",
    "        data[columns] = data[columns].replace(r'\\s+', ' ', regex=True)#remove white space in cell + line breaks from an excel cells\n",
    "        data.loc[:,columns] = data[columns].apply(lambda x: x.title() if isinstance(x, str) else x)\n",
    "        data[columns] = [re.sub('\\.0$', '', str(i)) if type(i) == float else i for i in data[columns]]#remove trailing .0\n",
    "        #data.loc[:,columns] = data[columns].apply(lambda x: x.title() if isinstance(x, str) else x)\n",
    "    for i in list_unformated_character:\n",
    "        data.replace(i, np.nan, inplace=True)\n",
    "    data = data.replace({np.nan: None})\n",
    "    return  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unicode_GapMap(data):\n",
    "    data = data.fillna('NULL')\n",
    "    data = data.apply(lambda x:deunicode_funtion(x) if x!=None else x)\n",
    "    data = data.replace('NULL',np.nan)\n",
    "    data = data.replace({np.nan: None})\n",
    "    return data.replace(r'\\s+', ' ', regex=True)#remove line breaks from an excel cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_Additional_Columns_GapMap(data,df_summ_file,BIHub,engine,file_url):\n",
    "        #Generate file Date\n",
    "    project_name=file_url.split('/')[-1]\n",
    "    history=df_summ_file[df_summ_file.Name==project_name]\n",
    "    File_Date=history.iloc[0,3]\n",
    "    data['File_Date']=File_Date\n",
    "    #Generate Import Date\n",
    "    today=pd.to_datetime('today').strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "    data['Import_Date']=today\n",
    "    #Generate Project Name\n",
    "    project_name=file_url.split('/')[-1]\n",
    "    project_name=project_name.split('.')[0]\n",
    "    data['File_Name']=project_name \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:32<00:00, 46.03s/it]\n"
     ]
    }
   ],
   "source": [
    "#Create empty df for checking dictionary\n",
    "df_dict = pd.DataFrame(columns=['File_Name', 'Missing_Values', 'Flag'])\n",
    "df_temp_flat = pd.DataFrame()\n",
    "df_flat = pd.DataFrame()\n",
    "#-------------------------------------------------------\n",
    "'''Get data''' \n",
    "\n",
    "columns_that_need_unidecode = []\n",
    "\n",
    "for file_url in tqdm(url):\n",
    "    data, file_name, sector = get_data(relative_url, file_url)\n",
    "    \n",
    "    SheetName = file_url.split('/')[-1].split('.')[0]\n",
    "    if SheetName == 'Commune':\n",
    "        table_name = 'GapMap_Commnue'\n",
    "        columns_that_need_unidecode=['Province','Ward','District']\n",
    "    elif SheetName == 'District':\n",
    "        table_name = 'GapMap_District'\n",
    "        columns_that_need_unidecode = ['Province','District']\n",
    "    data = remove_unformated_character_GapMap(data)\n",
    "    for i in columns_that_need_unidecode:\n",
    "        if i not in data.columns:\n",
    "            data[i] = np.nan\n",
    "            data[i] = data[i].replace({np.nan: None})\n",
    "            print(colored('Column {} is added in {}','yellow').format(i, f'{file_name}'))\n",
    "        else:\n",
    "            pass   \n",
    "        data[i] = remove_unicode(data[i])\n",
    "\n",
    "    \n",
    "    data = Generate_Additional_Columns_GapMap(data,df_summ_file,BIHub,engine,file_url)\n",
    "    data.to_sql(table_name, engine, index=False, if_exists='append', schema='Fresh')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
