{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from office365.runtime.auth.authentication_context import AuthenticationContext\n",
    "from office365.sharepoint.client_context import ClientContext\n",
    "from office365.sharepoint.files.file import File\n",
    "from office365.runtime.auth.client_credential import ClientCredential\n",
    "from office365.runtime.client_request_exception import ClientRequestException\n",
    "import datetime\n",
    "import pytz\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import glob\n",
    "import pyodbc\n",
    "import os \n",
    "import json\n",
    "from io import BytesIO\n",
    "import io\n",
    "import platform\n",
    "from function.PyToSp import *\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import quote_plus\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, event,text\n",
    "import pyodbc\n",
    "import requests\n",
    "import inspect\n",
    "from validate import *\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "from urllib.parse import quote_plus\n",
    "import msal\n",
    "from itertools import chain\n",
    "from send_email import *\n",
    "from Connection import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECTION SUCESSFUL!\n"
     ]
    }
   ],
   "source": [
    "def ConnectSharePoint(url_hub):\n",
    "    header_Hub = f\"share_point_{url_hub.split('/')[2].replace('-','')}\"\n",
    "    config_Hub = read_config_json(config_path, header_Hub)\n",
    "    Hub = SharePoint(config_Hub)\n",
    "    #Hub.check_connect()\n",
    "    return Hub\n",
    "\n",
    "def ConnectAzureSQLServer(): \n",
    "    f = open ('database_information.json', \"r\") #Database information file, can change information depending on the time\n",
    "    qq = json.loads(f.read())\n",
    "    f.close()\n",
    "    ini_cnt_str ='Driver={driver_str};Server=tcp:{servername},1433;database={database};Uid={username};Pwd={password};Encrypt=yes;Authentication=ActiveDirectoryPassword;Connection Timeout=30;'.format(**qq)\n",
    "    quoted = quote_plus(ini_cnt_str)\n",
    "    cnt_str = 'mssql+pyodbc:///?odbc_connect={}'.format(quoted)\n",
    "    engine = create_engine(cnt_str)\n",
    "\n",
    "        #Test Connection\n",
    "    try:\n",
    "        conn = engine.connect()\n",
    "        result = conn.execute(text(\"SELECT 1\"))\n",
    "        print(\"CONNECTION SUCESSFUL!\")\n",
    "    except Exception as e:\n",
    "        print(\"CONNECTION FAILED:\",str(e))\n",
    "    return cnt_str\n",
    "cnt_str = ConnectAzureSQLServer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_lenght(Sector, url_file):\n",
    "    excel_data = pd.DataFrame()\n",
    "    excel_data, file_name, sector = get_data(url_hub, url_file)\n",
    "    data_type = pd.read_excel('doc/DataType.xlsx', sheet_name= Sector)\n",
    "    # Iterate over the columns in the DataFrame\n",
    "    for column in excel_data.columns:\n",
    "        lenght_data = excel_data[column].astype(str).str.len().max()\n",
    "        for col in data_type['column_name']:\n",
    "            if col == column: \n",
    "                index = data_type[data_type['column_name'] == col].index[0]\n",
    "                max_length = data_type.loc[index, 'max_length']\n",
    "                if max_length != 'nan':\n",
    "                    if lenght_data > max_length:\n",
    "                        max_length = int(max_length)\n",
    "                        print(f'{col} có độ dài tối đa là: ', max_length)\n",
    "                        print(f'{column}: ', lenght_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_Template(Sector,url_hub, file_url):\n",
    "    # Read the first line of each file into separate dataframes\n",
    "    data, file_name, sector = get_data(url_hub, file_url)\n",
    "\n",
    "    df1 = pd.DataFrame(data)\n",
    "    df2 = pd.read_excel(f'doc/Template/Template_{Sector}.xlsx', header=None, nrows=1)\n",
    "\n",
    "    # Compare the values of the first lines\n",
    "    diff_values = {}\n",
    "    for i in range(len(df1.columns)):\n",
    "        if df1.columns[i] != df2.iloc[0, i]:\n",
    "            diff_values[i] = (df1.columns[i], df2.iloc[0, i])\n",
    "    return diff_values \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetProvinceIS(provinces, list_folder):  \n",
    "        selected_provinces = []\n",
    "        province_list = []\n",
    "        for i in list_folder:\n",
    "            province_list.append(i.split('/')[7])\n",
    "        for input_province in provinces:\n",
    "            input_province = input_province.strip().lower()\n",
    "            if input_province == \"-1\":\n",
    "                break\n",
    "\n",
    "            matched_provinces = [province for province in province_list if input_province in province.lower()]\n",
    "            if matched_provinces:\n",
    "                selected_provinces.extend(matched_provinces)\n",
    "            else:\n",
    "                print(f\"Không tìm thấy tỉnh chứa từ khóa '{input_province}'.\")\n",
    "        return selected_provinces \n",
    "def GetProvince(input_string, list_folder ,selected_provinces):  \n",
    "        input_list = input_string.split(',')\n",
    "        province_list = []\n",
    "        for i in list_folder:\n",
    "            province_list.append(i.split('/')[7])\n",
    "        for input_province in input_list:\n",
    "            input_province = input_province.strip().lower()\n",
    "            if input_province == \"-1\":\n",
    "                break\n",
    "\n",
    "            matched_provinces = [province for province in province_list if input_province in province.lower()]\n",
    "            if matched_provinces:\n",
    "                selected_provinces.extend(matched_provinces)\n",
    "            else:\n",
    "                print(f\"Không tìm thấy tỉnh chứa từ khóa '{input_province}'.\")\n",
    "        return selected_provinces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListImportFile(list_url):\n",
    "    url = []\n",
    "    for i in list_url:\n",
    "        sector = i.split('/')[-1].split('_')[0].upper()\n",
    "        if sector not in ('x'):\n",
    "            url.append(i)\n",
    "        else:\n",
    "            pass\n",
    "    return url\n",
    "def findFolderWithProvince(list_folder, selected_provinces  ,Year, Quarter):\n",
    "        # Tìm các folder chứa từ khóa\n",
    "        selected_province_folders = [folder for folder in list_folder if any(re.search(keyword, folder, re.IGNORECASE) for keyword in selected_provinces)]\n",
    "        selected_year_quarter = []\n",
    "        for folder in selected_province_folders:\n",
    "            folder_sub2 = folder + '/' + Year + '/' + Quarter\n",
    "            selected_year_quarter.append(folder_sub2)\n",
    "        # In danh sách các folder đã tìm thấy\n",
    "        return selected_year_quarter\n",
    "def findFolder(list_folder,selected_provinces , url_hub, year, quarter):\n",
    "        df_summ_file = pd.DataFrame({'Name':[],'ServerRelativeUrl':[], 'TimeLastModified':[], 'ModTime':[], 'Modified_by_ID':[]})\n",
    "        for i in findFolderWithProvince(list_folder, selected_provinces, year, quarter):\n",
    "            if i.split('/')[7] in selected_provinces and i.split('/')[8] in year and i.split('/')[9] in quarter:\n",
    "                df_summ_file = pd.concat([df_summ_file, ConnectSharePoint(url_hub).get_content_url(i)])\n",
    "        list_url = df_summ_file['ServerRelativeUrl'].to_list()\n",
    "        #History file\n",
    "        df_query=pd.DataFrame(df_summ_file).reset_index(drop=True)\n",
    "        df_summ_file = df_summ_file.reset_index(drop=True)\n",
    "        return list_url , df_summ_file\n",
    "def getFile(list_folder, selected_provinces, url_hub):\n",
    "    # Input provinces\n",
    "    input_string = input(\"Nhập tên các tỉnh bạn muốn chọn, cách nhau bằng dấu phẩy (nhập -1 để thoát): \")\n",
    "    # Input Năm và Quý\n",
    "    quarter_year = input(\"Nhập chuỗi năm và quý (ví dụ: 2023Q1): \")\n",
    "    Year, Quarter = quarter_year[:4], quarter_year[4:]\n",
    "\n",
    "    # In danh sách tỉnh đã chọn, năm và quý\n",
    "    print(\"Các tỉnh đã chọn:\")\n",
    "    for province in GetProvince(input_string, list_folder ,selected_provinces):\n",
    "        print(colored(province,'yellow'))\n",
    "    print(colored(\"Year:{}, Quarter:{}\".format(Year,Quarter),'yellow'))\n",
    "    findFolderWithProvince(list_folder, selected_provinces,Year,Quarter)\n",
    "    list_url , df_summ_file = findFolder(list_folder, selected_provinces, url_hub, Year, Quarter)\n",
    "    list_url = getListImportFile(list_url)\n",
    "    return list_url, df_summ_file\n",
    "def getFileMainSector(list_folder, selected_provinces, url_hub):\n",
    "    url = []\n",
    "    # Input provinces\n",
    "    input_string = input(\"Nhập tên các tỉnh bạn muốn chọn, cách nhau bằng dấu phẩy (nhập -1 để thoát): \")\n",
    "    # Input Năm và Quý\n",
    "    quarter_year = input(\"Nhập chuỗi năm và quý (ví dụ: 2023Q1): \")\n",
    "    Year, Quarter = quarter_year[:4], quarter_year[4:]\n",
    "\n",
    "    # In danh sách tỉnh đã chọn, năm và quý\n",
    "    print(\"Các tỉnh đã chọn:\")\n",
    "    for province in GetProvince(input_string, list_folder ,selected_provinces):\n",
    "        print(colored(province,'yellow'))\n",
    "    print(colored(\"Year:{}, Quarter:{}\".format(Year,Quarter),'yellow'))\n",
    "    findFolderWithProvince(list_folder, selected_provinces,Year,Quarter)\n",
    "    list_url , df_summ_file = findFolder(list_folder, selected_provinces, url_hub, Year, Quarter)\n",
    "    Subsector = input('Nhap Subsector: ')\n",
    "    for i in list_url:\n",
    "        sector = i.split('/')[-1].split('_')[0].upper()\n",
    "        if sector=='APARTMENT':\n",
    "            sector='APT'\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        if sector in (Subsector):#Chọn sector cần import: RETAIL, OFFICE, SA, HOTEL, APT, VLTH\n",
    "            url.append(i)\n",
    "        else:\n",
    "            pass\n",
    "    return url, df_summ_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (622098458.py, line 66)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 66\u001b[1;36m\u001b[0m\n\u001b[1;33m    engine = create_engine(cnt_str)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def validateIP(url_hub, file_url,cnt_str, sp_object,df_summ_file, Hub):\n",
    "    columns_that_need_unidecode=['Project_Name', 'Sub_Project_Name', 'Developer_Name'\n",
    "                                , 'City', 'District', 'Target_Industry']\n",
    "    engine = create_engine(cnt_str)\n",
    "    #Create empty df for checking dictionary\n",
    "    df_dict = pd.DataFrame(columns=['File_Name', 'Missing_Values', 'Flag'])\n",
    "    df_flat_ip = pd.DataFrame()\n",
    "    df_dup= pd.DataFrame()\n",
    "    df_temp_flat_ip = pd.DataFrame() \n",
    "    #Create multi empty df for tracking autdit step\n",
    "    name_sector = ['IP']\n",
    "    name_sector = [x.lower() for x in name_sector]\n",
    "    for i in name_sector:\n",
    "        globals()['df_temp_flat_{}'.format(i)] = pd.DataFrame([])\n",
    "        globals()['df_flat_{}'.format(i)] = pd.DataFrame([])\n",
    "        globals()['df_new_key_{}'.format(i)] = pd.DataFrame([])  \n",
    "    #-------------------------------------------------------\n",
    "    '''Get data''' \n",
    "    print(f'Start Get Data {file_url}')\n",
    "    data, file_name, sector = get_data(url_hub, file_url)\n",
    "    print(file_name, sector)\n",
    "    #-------------------------------------------------------\n",
    "    data = check_date_key(file_url, data)#Check format date_key in flat file   \n",
    "    data['Project_Name']= np.where(data['Project_Name'].isnull(), data['Sub_Project_Name'], data['Project_Name'])#Fill up project_name if its null\n",
    "    data['Project_Sub_Type']= np.where(data['Project_Sub_Type'].isnull(), data['Project_Type'], data['Project_Sub_Type'])#Fill up Sub_Type if its null\n",
    "\n",
    "    #Check duplicate sub_name\n",
    "    print('Start Check duplicate sub_name...')\n",
    "    data, df_dup = check_duplicate(data, 'Sub_Project_Name')\n",
    "    print(colored('Check duplicate sub_name', 'yellow'))\n",
    "    #-------------------------------------------------------\n",
    "    '''Validation step'''\n",
    "    print('Start validation step...')\n",
    "    #Remove unfortmated values\n",
    "    data = remove_unformated_character(data)\n",
    "    #Remove unicode characters\n",
    "    for i in columns_that_need_unidecode:\n",
    "        data[i] = remove_unicode(data[i])\n",
    "    #Check dictionary\n",
    "    print('Start Check dictionary...')\n",
    "    lst_dict = ['City', 'District', 'Status', 'Sub_Type', 'Type']\n",
    "    lst_cls = ['City', 'District', 'Status', 'Project_Sub_Type', 'Project_Type']\n",
    "    for i, j in zip(lst_cls, lst_dict):\n",
    "        print(f'Start checking dictionary of [{i}:{j}]...')\n",
    "        data, df_dict = check_dictionary(df_dict, file_name, data, i, j, sector, engine, sp_object)\n",
    "    print(f'End Check dictionary with df_dict = {len(df_dict)} ...')\n",
    "    \n",
    "    if len(df_dict) == 0:\n",
    "        print(colored('Validate succesfully','green'))\n",
    "    else:\n",
    "        #-------------------------------------------------------\n",
    "        '''Import data process'''\n",
    "        #Check project key\n",
    "        data = Generate_Additional_Columns(data,df_summ_file, Hub,engine,file_url)\n",
    "        processed_data, flag_key = check_project_key(file_url, data, sector, engine)\n",
    "        df_temp_flat_ip = pd.concat([df_temp_flat_ip, data], axis=0)\n",
    "        df_flat_ip = tracking_flat_file(df_temp_flat_ip, file_url)\n",
    "#         if len(processed_data) != 0:\n",
    "#             df_new_key_ip = check_new_key(df_new_key = df_new_key_ip, processed_data = processed_data, sector = sector)\n",
    "    return df_dup, df_dict, df_flat_ip, df_temp_flat_ip\n",
    "\n",
    "def validateMainSector(url_hub, file_url,cnt_str, sp_object,df_summ_file, Hub):\n",
    "    columns_that_need_unidecode = ['Project_Name','Sub_Project_Name','Developer','Constructor'\n",
    "                                    , 'Operator','Project_District_Name','Project_City_Name', 'Construction_Status']\n",
    "        engine = create_engine(cnt_str)\n",
    "        #Create empty df for checking dictionary\n",
    "        df_dict = pd.DataFrame(columns=['File_Name', 'Missing_Values', 'Flag'])\n",
    "        df_flat_ms = pd.DataFrame()\n",
    "        df_dup= pd.DataFrame()\n",
    "        #Create multi empty df for tracking autdit step\n",
    "        name_sector = ['RETAIL', 'SA', 'APT', 'VLTH', 'OFFICE', 'HOTEL']\n",
    "        name_sector = [x.lower() for x in name_sector]\n",
    "        for i in name_sector:\n",
    "            globals()['df_temp_flat_{}'.format(i)] = pd.DataFrame([])\n",
    "            globals()['df_flat_{}'.format(i)] = pd.DataFrame([]) \n",
    "            globals()['df_new_key_{}'.format(i)] = pd.DataFrame([])  \n",
    "        #-------------------------------------------------------\n",
    "        '''Get data''' \n",
    "        for file_url in tqdm(list_url):  \n",
    "            data, file_name, sector = get_data(url_hub, file_url)\n",
    "            #-------------------------------------------------------\n",
    "            data = check_date_key(file_url, data) #Check format date_key in flat file   \n",
    "            data['Project_Name']= np.where(data['Project_Name'].isnull(), data['Sub_Project_Name'], data['Project_Name']) #Fill up project_name if its null\n",
    "            #Check duplicate sub_name\n",
    "            data, df_dup = check_duplicate(data, 'Sub_Project_Name')\n",
    "            if len(df_dup) != 0:\n",
    "                print(colored('Check dupplicate sub_name', 'yellow'))\n",
    "                df_noti_html = convert_df_to_html(type_html = 1, df = df_dup, type_sector = 1, cnxn = engine)\n",
    "                to_email = ['nthieu@savills.com.vn', 'hcmcbi-intern04@savills.com.vn']\n",
    "                run_email(type_sector = 'MAIN SECTOR', email_type = 1, user_email = to_email, df_noti_html = df_noti_html)\n",
    "            else:       \n",
    "                #-------------------------------------------------------\n",
    "                '''Validation step'''\n",
    "                #Remove unfortmated values\n",
    "                #data = remove_unformated_character(type_sector = 1, sector = sector, file_name = file_name, data = data)\n",
    "                data = remove_unformated_character(data)\n",
    "                #Remove unicode characters\n",
    "                for i in columns_that_need_unidecode:\n",
    "                    if i not in data.columns:\n",
    "                        data[i] = np.nan\n",
    "                        data[i] = data[i].replace({np.nan: None})\n",
    "                        print(colored('Column {} is added in {}','yellow').format(i, f'{file_name}'))\n",
    "                    else:\n",
    "                        pass   \n",
    "                    data[i] = remove_unicode(data[i])\n",
    "                #Check dictionary\n",
    "                # lst_dict = ['City', 'District', 'Status', 'Type', 'Grade']\n",
    "                # lst_cls = ['Project_City_Name', 'Project_District_Name', 'Project_Status', 'Sub_Project_Type', 'Grade']`\n",
    "                lst_dict = ['City', 'District', 'Status', 'Type']\n",
    "                lst_cls = ['Project_City_Name', 'Project_District_Name', 'Project_Status', 'Sub_Project_Type']\n",
    "                print(f'Start check dictionary {file_name}')\n",
    "                for i, j in zip(lst_cls, lst_dict):\n",
    "                    \n",
    "                    data, df_dict = check_dictionary(df_dict, file_name, data, i, j, sector, engine, sp_object)\n",
    "                    \n",
    "                    \n",
    "                print(df_dict)\n",
    "                if len(df_dict) == 0:\n",
    "                    print(colored('Validate succesfully','green'))\n",
    "                    #-------------------------------------------------------\n",
    "                    '''Import data process'''\n",
    "                    #Check project key\n",
    "                    data = Generate_Additional_Columns(data,df_summ_file,Hub,engine,file_url)\n",
    "                    processed_data, flag_key = check_project_key(file_url, data, sector, engine)\n",
    "                    #Create tracking audit\n",
    "                    if sector == 'RETAIL':\n",
    "                        df_temp_flat_retail = pd.DataFrame()\n",
    "                        df_temp_flat_retail = pd.concat([df_temp_flat_retail, data], axis=0)\n",
    "                        df_flat_ms=tracking_flat_file(df_temp_flat_retail, file_url)\n",
    "                        if len(processed_data) != 0:\n",
    "                            df_new_key_retail= pd.DataFrame()\n",
    "                            df_new_key_retail = check_new_key(df_new_key = df_new_key_retail, processed_data = processed_data, sector = sector)\n",
    "                    elif sector == 'OFFICE':\n",
    "                        df_temp_flat_office = pd.DataFrame()\n",
    "                        df_temp_flat_office = pd.concat([df_temp_flat_office, data], axis=0)\n",
    "                        df_flat_ms = tracking_flat_file(df_temp_flat_office, file_url)\n",
    "                        if len(processed_data) != 0:\n",
    "                            df_new_key_office= pd.DataFrame()\n",
    "                            df_new_key_office = check_new_key(df_new_key = df_new_key_office, processed_data = processed_data, sector = sector)\n",
    "                    elif sector == 'HOTEL':\n",
    "                        print('Hotel')\n",
    "                        df_temp_flat_hotel = pd.DataFrame()\n",
    "                        df_temp_flat_hotel = pd.concat([df_temp_flat_hotel, data], axis=0)\n",
    "                        df_flat_ms = tracking_flat_file(df_temp_flat_hotel, file_url)\n",
    "                        if len(processed_data) != 0:\n",
    "                            df_new_key_hotel= pd.DataFrame()\n",
    "                            df_new_key_hotel = check_new_key(df_new_key = df_new_key_hotel, processed_data = processed_data, sector = sector)\n",
    "                    elif sector == 'SA' or sector=='SERVICED_APARTMENT':\n",
    "                        df_temp_flat_sa = pd.DataFrame()\n",
    "                        df_temp_flat_sa = pd.concat([df_temp_flat_sa, data], axis=0)\n",
    "                        df_flat_ms = tracking_flat_file(df_temp_flat_sa, file_url)\n",
    "                        if len(processed_data) != 0:\n",
    "                            df_new_key_sa= pd.DataFrame()\n",
    "                            df_new_key_sa = check_new_key(df_new_key = df_new_key_sa, processed_data = processed_data, sector = sector)\n",
    "                    elif sector == 'APT' or sector=='APARTMENT':\n",
    "                        df_temp_flat_apt = pd.DataFrame()\n",
    "                        df_temp_flat_apt = pd.concat([df_temp_flat_apt, data], axis=0)\n",
    "                        df_flat_ms = tracking_flat_file(df_temp_flat_apt, file_url)\n",
    "                        if len(processed_data) != 0:\n",
    "                            df_new_key_apt= pd.DataFrame()\n",
    "                            df_new_key_apt = check_new_key(df_new_key = df_new_key_apt, processed_data = processed_data, sector = sector)\n",
    "                    \n",
    "                    elif sector == 'VLTH':\n",
    "                        df_temp_flat_vlth = pd.DataFrame()\n",
    "                        df_temp_flat_vlth = pd.concat([df_temp_flat_vlth, data], axis=0)\n",
    "                        df_flat_ms = tracking_flat_file(df_temp_flat_vlth, file_url)\n",
    "                        if len(processed_data) != 0:\n",
    "                            df_new_key_vlth= pd.DataFrame()\n",
    "                            df_new_key_vlth = check_new_key(df_new_key = df_new_key_vlth, processed_data = processed_data, sector = sector) \n",
    "                    else:\n",
    "                        pass\n",
    "                    #Get key and generate new key (if needed)\n",
    "                else:\n",
    "                    pass\n",
    "    return df_dup, df_dict, df_flat_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dup= pd.DataFrame()\n",
    "df_dict= pd.DataFrame()\n",
    "df_flat_ip = pd.DataFrame()\n",
    "df_temp_flat_ip= pd.DataFrame()\n",
    "diff_values ={}\n",
    "selected_provinces = [] \n",
    "while(True):\n",
    "        Sector= input(\"Nhập Sector bạn muốn lấy data: \")\n",
    "        url_hub=f'/sites/BIHub/Shared Documents/Advisory Data/{Sector}/Flat file'\n",
    "        Hub= ConnectSharePoint(url_hub)\n",
    "        sp_object = url_hub.split('/')[2].replace('-','')\n",
    "        list_folder = Hub.get_content_url(url_hub, return_list_folder=True)\n",
    "        if(Sector == 'Main Sector'):\n",
    "            url, df_summ_file = getFileMainSector(list_folder, selected_provinces, url_hub)\n",
    "            for i in url: \n",
    "                sector = i.split('/')[-1].split('_')[0].upper()\n",
    "                diff_values = compare_Template(sector,url_hub, i)\n",
    "                check_data_lenght(sector, i)\n",
    "                df_dup, df_dict,df_flat_ip, df_temp_flat_ip= validateMainSector(url_hub, i,cnt_str, sp_object,df_summ_file, Hub)\n",
    "        if (Sector == 'IP'):\n",
    "            url, df_summ_file = getFile(list_folder, selected_provinces, url_hub)\n",
    "            for i in url: \n",
    "                diff_values = compare_Template(Sector,url_hub, i)\n",
    "                check_data_lenght(Sector, i)\n",
    "                df_dup, df_dict,df_flat_ip, df_temp_flat_ip= validateIP(url_hub, i,cnt_str, sp_object,df_summ_file, Hub)\n",
    "        if (Sector == 'Infra'):\n",
    "            url, df_summ_file = getFile(list_folder, selected_provinces, url_hub)\n",
    "            for i in url: \n",
    "                diff_values = compare_Template(Sector,url_hub, i)\n",
    "                check_data_lenght(Sector, i)\n",
    "                df_dup, df_dict,df_flat_ip, df_temp_flat_ip= validateIP(url_hub, i,cnt_str, sp_object,df_summ_file, Hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check column name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Different values:\")\n",
    "for col, (val1, val2) in diff_values.items():\n",
    "    print(f\"Column {col+1}: {val1} != {val2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat_ip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check with Sub_Project_Name\n",
    "If there are 2 duplicate Sub_Project_Name, an error will be displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
