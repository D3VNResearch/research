{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f9c70bc",
   "metadata": {},
   "source": [
    "### IMPORT LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccfa4d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from office365.runtime.auth.authentication_context import AuthenticationContext\n",
    "from office365.sharepoint.client_context import ClientContext\n",
    "from office365.sharepoint.files.file import File\n",
    "from office365.runtime.auth.client_credential import ClientCredential\n",
    "from office365.runtime.client_request_exception import ClientRequestException\n",
    "import datetime\n",
    "import pytz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import glob\n",
    "import pyodbc\n",
    "import os \n",
    "import json\n",
    "from io import BytesIO\n",
    "import io\n",
    "import platform\n",
    "from function.PyToSp import *\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import quote_plus\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, event, text\n",
    "import pyodbc\n",
    "import requests\n",
    "import inspect\n",
    "from validate import *\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "from urllib.parse import quote_plus\n",
    "import msal\n",
    "from itertools import chain\n",
    "from send_email import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bee114",
   "metadata": {},
   "source": [
    "### CONNECT TO AZURE SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a39960d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECTION SUCESSFUL!\n"
     ]
    }
   ],
   "source": [
    "def ConnectAzureSQLServer(): \n",
    "    f = open ('database_information.json', \"r\") #Database information file, can change information depending on the time\n",
    "    qq = json.loads(f.read())\n",
    "    f.close()\n",
    "    ini_cnt_str ='Driver={driver_str};Server=tcp:{servername},1433;database={database};Uid={username};Pwd={password};Encrypt=yes;Authentication=ActiveDirectoryPassword;Connection Timeout=30;'.format(**qq)\n",
    "    quoted = quote_plus(ini_cnt_str)\n",
    "    cnt_str = 'mssql+pyodbc:///?odbc_connect={}'.format(quoted)\n",
    "    engine = create_engine(cnt_str)\n",
    "\n",
    "        #Test Connection\n",
    "    try:\n",
    "        conn = engine.connect()\n",
    "        result = conn.execute(text(\"SELECT 1\"))\n",
    "        print(\"CONNECTION SUCESSFUL!\")\n",
    "    except Exception as e:\n",
    "        print(\"CONNECTION FAILED:\",str(e))\n",
    "    return cnt_str\n",
    "cnt_str = ConnectAzureSQLServer()\n",
    "engine = create_engine(cnt_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81420039",
   "metadata": {},
   "source": [
    "### CONNECT TO SHAREPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b0587ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web site title: S22M Research & BI Hub _ Successful Connection!\n"
     ]
    }
   ],
   "source": [
    "header_BIHub = 'share_point_BIHub'\n",
    "config_BIHub = read_config_json(config_path, header_BIHub)\n",
    "BIHub = SharePoint(config_BIHub)\n",
    "BIHub.check_connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d951e70e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder name: /sites/BIHub/Shared Documents/Advisory Data/FDI/Flat file/2021\n",
      "/sites/BIHub/Shared Documents/Advisory Data/FDI/Flat file/2018\n",
      "/sites/BIHub/Shared Documents/Advisory Data/FDI/Flat file/2019\n",
      "/sites/BIHub/Shared Documents/Advisory Data/FDI/Flat file/2022\n",
      "/sites/BIHub/Shared Documents/Advisory Data/FDI/Flat file/2016\n",
      "/sites/BIHub/Shared Documents/Advisory Data/FDI/Flat file/2020\n",
      "/sites/BIHub/Shared Documents/Advisory Data/FDI/Flat file/2023\n",
      "/sites/BIHub/Shared Documents/Advisory Data/FDI/Flat file/2017\n"
     ]
    }
   ],
   "source": [
    "#Tất cả các tỉnh\n",
    "relative_url = \"/sites/BIHub/Shared Documents/Advisory Data/FDI/Flat file\"\n",
    "sp_object = relative_url.split('/')[2].replace('-','')\n",
    "list_folder = eval(sp_object).get_content_url(relative_url,return_list_folder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "955b3c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder name: /sites/BIHub/Shared Documents/Advisory Data/FDI/Flat file/2021/Q4\n",
      "Folder name: /sites/BIHub/Shared Documents/Advisory Data/FDI/Flat file/2018/Q4\n",
      "Folder name: /sites/BIHub/Shared Documents/Advisory Data/FDI/Flat file/2019/Q4\n",
      "Folder name: /sites/BIHub/Shared Documents/Advisory Data/FDI/Flat file/2022/Q3\n",
      "/sites/BIHub/Shared Documents/Advisory Data/FDI/Flat file/2022/Q4\n",
      "/sites/BIHub/Shared Documents/Advisory Data/FDI/Flat file/2022/Q2\n",
      "/sites/BIHub/Shared Documents/Advisory Data/FDI/Flat file/2022/Q1\n",
      "Folder name: /sites/BIHub/Shared Documents/Advisory Data/FDI/Flat file/2016/Q4\n",
      "Folder name: /sites/BIHub/Shared Documents/Advisory Data/FDI/Flat file/2020/Q4\n",
      "Folder name: /sites/BIHub/Shared Documents/Advisory Data/FDI/Flat file/2023/Q2\n",
      "/sites/BIHub/Shared Documents/Advisory Data/FDI/Flat file/2023/Q1\n",
      "Folder name: /sites/BIHub/Shared Documents/Advisory Data/FDI/Flat file/2017/Q4\n"
     ]
    }
   ],
   "source": [
    "#lấy năm\n",
    "list_folder_sub1=[]\n",
    "for i in list_folder:\n",
    "    ls=eval(sp_object).get_content_url(i,return_list_folder=True)\n",
    "    list_folder_sub1=list_folder_sub1+ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa46fe3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder name: \n",
      "Files name: /sites/BIHub/Shared Documents/Advisory Data/FDI/Flat file/2023/Q1/FDI_20230331.xlsx\n"
     ]
    }
   ],
   "source": [
    "quarter = ['Q1']#Chọn năm cần import\n",
    "year = ['2023']#Chọn quý cần import\n",
    "#-------------------------------------------------------\n",
    "df_summ_file = pd.DataFrame({'Name':[],'ServerRelativeUrl':[], 'TimeLastModified':[], 'ModTime':[], 'Modified_by_ID':[]})\n",
    "for i in list_folder_sub1:\n",
    "    if i.split('/')[-2] in year and i.split('/')[-1] in quarter:\n",
    "        df_summ_file = pd.concat([df_summ_file, eval(sp_object).get_content_url(i)])\n",
    "list_file = df_summ_file['ServerRelativeUrl'].to_list()\n",
    "#History file\n",
    "df_query=pd.DataFrame(df_summ_file).reset_index(drop=True)\n",
    "df_summ_file = df_summ_file.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5bc3144",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/sites/BIHub/Shared Documents/Advisory Data/FDI/Flat file/2023/Q1/FDI_20230331.xlsx']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = []\n",
    "for i in list_file:\n",
    "    data_key = i.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "    if str(data_key) not in ('x'):\n",
    "        url.append(i)\n",
    "url        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f2ceebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_check_dictionary(df_dict, file_name, data, column_name, parameter, sector, cnxn, sp_object):\n",
    "    lower_function = lambda x: x.strip().lower() if isinstance(x, str) else x\n",
    "\n",
    "    df_raw=pd.DataFrame()\n",
    "    if parameter in ['City', 'District', 'Status', 'Indicator'\n",
    "                     , 'Country', 'Industry'\n",
    "                    ]:\n",
    "        raw_parameter = pd.read_sql(f'select * from GENERAL.{parameter}_Dictionary',cnxn)\n",
    "        raw_parameter[f'Raw_{parameter}'] = raw_parameter[f'Raw_{parameter}'].apply(lower_function)\n",
    "    elif parameter == 'Grade':\n",
    "        if sector in ['HOTEL', 'OFFICE', 'APARTMENT'\n",
    "                      , 'APT', 'SA', 'SERVICED_APARTMENT'\n",
    "                     ]:\n",
    "            raw_parameter = pd.read_sql(f\"select * from GENERAL.Project_{parameter}_Dictionary\",cnxn)\n",
    "            raw_parameter[f'Raw_{parameter}'] = raw_parameter[f'Raw_{parameter}'].apply(lower_function)\n",
    "        else:\n",
    "            return data, df_dict\n",
    "    elif parameter in ['Type', 'Sub_Type']:\n",
    "        if sector in ['RETAIL', 'VLTH', 'APARTMENT'\n",
    "                      , 'APT', 'IP', 'TENANT'\n",
    "                     ]:\n",
    "            raw_parameter = pd.read_sql(f\"select * from GENERAL.Project_{parameter}_Dictionary WHERE Sector = '{sector}' \",cnxn)\n",
    "            raw_parameter[f'Raw_{parameter}'] = raw_parameter[f'Raw_{parameter}'].apply(lower_function)\n",
    "        else:\n",
    "            return data, df_dict\n",
    "    elif parameter in ['Project_Name', 'Investor_Nationality', 'Industry_Lv1', 'Investment_Form'\n",
    "                      , 'Infrastructure_Level_1', 'Infrastructure_Level_2'\n",
    "                      ]:\n",
    "        if sector in ['FDI', 'INFRA']:\n",
    "            header_BIHub = 'share_point_BIHub'\n",
    "            config_BIHub = read_config_json(config_path, header_BIHub)\n",
    "            BIHub = SharePoint(config_BIHub)\n",
    "            raw_parameter = eval(sp_object).get_file(f'/sites/BIHub/Shared Documents/Advisory Data/{sector}/Mapping/{parameter}_Dictionary.xlsx')\n",
    "            raw_parameter[f'Raw_{parameter}'] = remove_unicode(raw_parameter[f'Raw_{parameter}']).apply(lower_function)\n",
    "            raw_parameter = raw_parameter.drop_duplicates(subset=[f'Raw_{parameter}'], keep='last')\n",
    "        else:\n",
    "            return data, df_dict \n",
    "    else:\n",
    "        print(colored('Unknown parameter in check dictionary section','yellow'))\n",
    "        return data, df_dict \n",
    "    df_raw.to_csv('raw.csv', index = False)\n",
    "    data[f'{column_name}']= remove_unicode(data[f'{column_name}']).apply(lower_function)\n",
    "    # data[f'Convert_{parameter}'] = pd.merge(data, raw_parameter\n",
    "    #                                         , how='left'\n",
    "    #                                         , left_on=f'{column_name}'\n",
    "    #                                         , right_on=f'Raw_{parameter}')[f'Cleaned_{parameter}']\n",
    "    # Hieu update - 31-05-2023\n",
    "    get_cleaned_type = lambda x: raw_parameter.loc[raw_parameter[f'Raw_{parameter}'] == x, f'Cleaned_{parameter}'].values[0] if len(raw_parameter.loc[raw_parameter[f'Raw_{parameter}'] == x, f'Cleaned_{parameter}']) > 0 else None\n",
    "    #get_cleaned_type = lambda x: raw_parameter.loc[raw_parameter[f'Raw_{parameter}'] == x,f'Cleaned_{parameter}'].values[0] #fix lỗi duplicate của hàm merge trên\\\n",
    "    data[f'Convert_{parameter}'] = data[f'{column_name}'].map(get_cleaned_type)\n",
    "    # print(data[f'Convert_{parameter}'])\n",
    "    ## End - Hieu update - 31-05-2023\n",
    "    parameter_not_in_dict = data[f'{column_name}'][data[f'Convert_{parameter}'].isnull()]\n",
    "    if len(parameter_not_in_dict) != 0: \n",
    "        temp_df = pd.DataFrame()\n",
    "        parameter_not_in_dict = list(set(parameter_not_in_dict))\n",
    "        temp_df.insert(loc=0, column = 'Missing_Values', value = parameter_not_in_dict)\n",
    "        temp_df.insert(loc=0, column = 'File_Name', value = f'{file_name}')\n",
    "        temp_df.insert(loc=0, column = 'Flag', value = f'{parameter}')\n",
    "        df_dict = pd.concat([df_dict, temp_df], axis=0)\n",
    "        return data, df_dict\n",
    "    else:\n",
    "        if parameter == 'Project_Name':\n",
    "            data[f'{column_name}'] = data[f'{column_name}'].str.title()\n",
    "            data[f'Convert_{parameter}'] = data[f'Convert_{parameter}'].str.title()\n",
    "            data.rename(columns = {f'Convert_{parameter}':f'{parameter}_Eng'}, inplace = True)\n",
    "        else:\n",
    "            data[f'{column_name}'] = data[f'Convert_{parameter}']\n",
    "            data = data.drop(columns = [f'Convert_{parameter}'])\n",
    "        return data, df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc83b84",
   "metadata": {},
   "source": [
    "### IMPORT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51d50056",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/1 [00:18<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mValidate succesfully\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Engine' object has no attribute '_instantiate_plugins'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m     df_temp_flat_fdi \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_temp_flat_fdi, data], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     31\u001b[0m     df_flat_fdi \u001b[38;5;241m=\u001b[39m tracking_flat_file(df_temp_flat_fdi, sector)\n\u001b[1;32m---> 32\u001b[0m     \u001b[43minsert_to_fresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Project_Savills\\Code Cu~\\research\\before_202306\\validate.py:1171\u001b[0m, in \u001b[0;36minsert_to_fresh\u001b[1;34m(file_url, data, cnt_str)\u001b[0m\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minsert_to_fresh\u001b[39m(file_url, data, cnt_str):\n\u001b[0;32m   1167\u001b[0m     \u001b[38;5;66;03m# conn ='Driver={ODBC Driver 17 for SQL Server};Server=tcp:hkazdevsqld3vnreserch.database.windows.net,1433;Database=D3VNResearch_Staging;Uid=D3VNResearch@savills.com.vn;Pwd=@Advisory032023!;Encrypt=yes;Authentication=ActiveDirectoryPassword;Connection Timeout=30;'\u001b[39;00m\n\u001b[0;32m   1168\u001b[0m     \u001b[38;5;66;03m# quoted = quote_plus(conn)\u001b[39;00m\n\u001b[0;32m   1169\u001b[0m     \u001b[38;5;66;03m# engine=create_engine('mssql+pyodbc:///?odbc_connect={}'.format(quoted))\u001b[39;00m\n\u001b[1;32m-> 1171\u001b[0m     engine \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnt_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;66;03m#Test Connection\u001b[39;00m\n\u001b[0;32m   1173\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m<string>:2\u001b[0m, in \u001b[0;36mcreate_engine\u001b[1;34m(url, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\sqlalchemy\\util\\deprecations.py:375\u001b[0m, in \u001b[0;36mdeprecated_params.<locals>.decorate.<locals>.warned\u001b[1;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m    369\u001b[0m         _warn_with_version(\n\u001b[0;32m    370\u001b[0m             messages[m],\n\u001b[0;32m    371\u001b[0m             versions[m],\n\u001b[0;32m    372\u001b[0m             version_warnings[m],\n\u001b[0;32m    373\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m    374\u001b[0m         )\n\u001b[1;32m--> 375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\create.py:516\u001b[0m, in \u001b[0;36mcreate_engine\u001b[1;34m(url, **kwargs)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;66;03m# create url.URL object\u001b[39;00m\n\u001b[0;32m    514\u001b[0m u \u001b[38;5;241m=\u001b[39m _url\u001b[38;5;241m.\u001b[39mmake_url(url)\n\u001b[1;32m--> 516\u001b[0m u, plugins, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_instantiate_plugins\u001b[49m(kwargs)\n\u001b[0;32m    518\u001b[0m entrypoint \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39m_get_entrypoint()\n\u001b[0;32m    519\u001b[0m dialect_cls \u001b[38;5;241m=\u001b[39m entrypoint\u001b[38;5;241m.\u001b[39mget_dialect_cls(u)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Engine' object has no attribute '_instantiate_plugins'"
     ]
    }
   ],
   "source": [
    "'''Prepare ingredients'''\n",
    "columns_that_need_unidecode=['Project_Name', 'Business_Name', 'Investor_Name', 'Investor_Nationality'\n",
    "                             , 'Industry_Lv1', 'Investment_Form', 'City', 'District']\n",
    "#Create empty df for checking dictionary\n",
    "df_dict = pd.DataFrame(columns=['File_Name', 'Missing_Values', 'Flag'])\n",
    "df_temp_flat_fdi = pd.DataFrame()\n",
    "df_flat_fdi = pd.DataFrame()\n",
    "#-------------------------------------------------------\n",
    "'''Get data''' \n",
    "for file_url in tqdm(url):\n",
    "    data, file_name, sector = get_data(relative_url, file_url)\n",
    "    #-------------------------------------------------------\n",
    "    data = check_date_key(file_url, data)#Check format date_key in flat file\n",
    "    #-------------------------------------------------------\n",
    "    '''Validation step'''\n",
    "    #Remove unfortmated values\n",
    "    #data = remove_unformated_character(type_sector = 2, sector = sector, file_name = file_name, data = data)\n",
    "    data = remove_unformated_character(data)\n",
    "    #Remove unicode characters\n",
    "    for i in columns_that_need_unidecode:\n",
    "        data[i] = remove_unicode(data[i])\n",
    "    #Check dictionary\n",
    "    lst_dict = ['City', 'District', 'Project_Name', 'Investor_Nationality', 'Industry_Lv1', 'Investment_Form']\n",
    "    lst_cls = ['City', 'District', 'Project_Name', 'Investor_Nationality', 'Industry_Lv1', 'Investment_Form']\n",
    "    for i, j in zip(lst_cls, lst_dict):\n",
    "        data, df_dict = check_dictionary(df_dict, file_name, data, i, j, sector, engine, sp_object)\n",
    "    if len(df_dict) == 0:\n",
    "        print(colored('Validate succesfully','green'))\n",
    "        data = Generate_Additional_Columns(data,url,df_summ_file,BIHub,engine,file_url)\n",
    "        df_temp_flat_fdi = pd.concat([df_temp_flat_fdi, data], axis=0)\n",
    "        df_flat_fdi = tracking_flat_file(df_temp_flat_fdi, sector)\n",
    "        insert_to_fresh(file_url, data, cnt_str)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab4b79b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### SEND EMAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8736012f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_df_flat = [df_flat_fdi]\n",
    "if len(df_dict) != 0:\n",
    "    print(colored('Missing values in dictionary','yellow'))\n",
    "    df_noti_html = convert_df_to_html(type_html = 2, df = df_dict, cnxn = engine)\n",
    "    to_email = ['hcmcbi-intern04@savills.com.vn']\n",
    "    run_email(email_type = 2, user_email = to_email, df_noti_html = df_noti_html)\n",
    "else:\n",
    "    print('Imported data succesfully')\n",
    "    df_flat_html, df_query_html = convert_df_to_html(type_html = 3, list_df = list_df_flat, type_sector = 2, cnxn = engine)\n",
    "    to_email = ['hcmcbi-intern04@savills.com.vn']\n",
    "    run_email(type_sector = 'FDI', email_type = 3, user_email = to_email, df_flat_html = df_flat_html, df_query_html = df_query_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a8fdb2",
   "metadata": {},
   "source": [
    "### TRACKING AUDIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d522b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_to_tracking(list_df_flat, 'Tracking_FDI', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a4adc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
